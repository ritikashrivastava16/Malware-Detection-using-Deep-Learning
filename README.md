# Malware Detection using Deep Learning

## tl;dr
1) Collected benign and malicious executable (.exe) files from different sources
2) Disassembled executable files into assembly (.asm) language files
3) Extracted opcode sequences from .asm files using a Python script (extract_malware.py, extract_benign.py)
4) Cleaned and pre-processed the opcode sequences (curate_dataset.py)
5) Split the data into train and test set
6) Fit a tokenizer to the training data and used the same tokenizer to transform test data to form tokenized opcode sequence matrix
7) Used Word Embedding technology to automatically obtain feature vector representation (much like NLP, this technology helps in understanding the semantic context of opcodes) 
8) Developed an LSTM model with 3 layers of 32 neurons each. Needed a sequence model to automatically learn opcode sequences to distinguish benign file from malicious file (chose this over RNN because RNN suffers from problem of exploding and vanishing gradients)
Added dropout layers to prevent overfitting. 
Used adam optimizer with 0.001 learning rate. 
9) Trained the model and tested it using the accuracy metric
10) Performed 10-fold cross validation to yield 95.01% (+/- 2.40%) accuracy
11) Compared output with various Machine Learning algorithms and Deep Learning Models

## Problem Statement
<p>Most malware detection systems identify malicious code by monitoring its execution in a sandbox environment to detect anomalous behavior. Discovering the above anomalous behavior is an indication of malware and is an important part of malware detection. Most sandboxes and next-generation firewalls rely heavily or even exclusively on this approach to identify malicious objects.
We need a better and more comprehensive approach. A major goal of malware detection is to capture additional properties to be used to improve
security measures and make evasion as difficult as possible. </p>
<p>Malware detection is a growing research area because of concern over increase of malwares on a daily basis. In this project we have used a dataset of 821 files where the number of malware files is 621 and the number of benign files is 200. We realised that opcode sequences can be learnt using the Word Embedding technology which can further distinguish the nature of the files. By using the collated dataset, we have built numerous Machine Learning and Deep Learning models such as SVM, XGBoost and LSTMs.</p>
<p>We have used the Long Short term memory model, a type of Recurrent Neural Network (RNN), to predict the nature of an executable file.</p>


## Dataset
<p>Dataset contains 621 malware and 200 benign binaries files. Malware files which are divided into 5 types: Locker, Mediyes , Winwebsec , Zbot , Zeroaccess. All malware files are collected from https://virusshare.com/ and malicia-project.com. Benign executable files are taken from installed folders of applications of legitimate software from different categories. They can be downloaded from https://download.cnet.com/windows/. All the files are verified by VirusTotal (https://www.virustotal.com) to make sure each file belongs to their type.</p>

## Component Design
The Malware Detection Methodology proposed for this project can be divided into three sections/components:
1. Data Pre-processing Section
2. Classification Model Section

![image](https://user-images.githubusercontent.com/54627261/207661978-4f6117ad-baf8-4213-9a97-d2b3124dff33.png)

## Classification Model
A total of 820 files are used in this project. Out of these 821 files, 621 files belong to the malware category and 200 files belong to the benign category.
<br>The classification is done using a 3 Layer LSTM model. Initially the dataframe is divided into two variables namely X and y. The X input variable contains the sequence of opcodes and the y input variable contains the opcode label (0 or 1). The X and y variables are split into train and test sets as 90% and 10% of the dataset respectively.
<br>The Tokenizer is used to label all the unique opcodes that are present in different file sequences. It is used to vectorize a text corpus, by turning each text into either a sequence of integers. This encoded sequence replaces the actual opcode with a number designated to that particular code by the Tokenizer.
<br>We have limited the number of words in the corpus to 1000 and are using 54580 as the length of each opcode sequence since it lies at the 90th percentile of the data. By using the 90th percentile, we are capturing maximum information without adding extra computational costs.

![image](https://user-images.githubusercontent.com/54627261/207663121-ce337d8b-541e-4aa6-948c-e6252c583054.png)

## Model Evaluation

![image](https://user-images.githubusercontent.com/54627261/207664573-abf28f0d-2bf4-4109-b6fd-05b50bdc887b.png)

![image](https://user-images.githubusercontent.com/54627261/207664644-2222885a-adc9-49ea-9ed2-afc470bfcc51.png)

- **10-fold CV: 95.01% (+/- 2.40%)**
- **5-fold CV: 95.62% (+/- 0.80%)**
<br>**These results validate the testing accuracy of 95% we have achieved from the 3 Layer LSTM Model.**

